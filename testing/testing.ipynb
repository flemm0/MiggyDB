{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>,\n",
      "            {'Alan': [(18, 'Alan'), (28, 'Alan')],\n",
      "             'Glory': [(28, 'Glory')],\n",
      "             'Jonah': [(27, 'Jonah')],\n",
      "             'Popeye': [(18, 'Popeye')]})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>age</th><th>name_l</th><th>name_r</th><th>word</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>27</td><td>&quot;Jonah&quot;</td><td>&quot;Jonah&quot;</td><td>&quot;Whales&quot;</td></tr><tr><td>27</td><td>&quot;Jonah&quot;</td><td>&quot;Jonah&quot;</td><td>&quot;Spiders&quot;</td></tr><tr><td>18</td><td>&quot;Alan&quot;</td><td>&quot;Alan&quot;</td><td>&quot;Ghosts&quot;</td></tr><tr><td>28</td><td>&quot;Alan&quot;</td><td>&quot;Alan&quot;</td><td>&quot;Ghosts&quot;</td></tr><tr><td>18</td><td>&quot;Alan&quot;</td><td>&quot;Alan&quot;</td><td>&quot;Zombies&quot;</td></tr><tr><td>28</td><td>&quot;Alan&quot;</td><td>&quot;Alan&quot;</td><td>&quot;Zombies&quot;</td></tr><tr><td>28</td><td>&quot;Glory&quot;</td><td>&quot;Glory&quot;</td><td>&quot;Buffy&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7, 4)\n",
       "┌─────┬────────┬────────┬─────────┐\n",
       "│ age ┆ name_l ┆ name_r ┆ word    │\n",
       "│ --- ┆ ---    ┆ ---    ┆ ---     │\n",
       "│ i64 ┆ str    ┆ str    ┆ str     │\n",
       "╞═════╪════════╪════════╪═════════╡\n",
       "│ 27  ┆ Jonah  ┆ Jonah  ┆ Whales  │\n",
       "│ 27  ┆ Jonah  ┆ Jonah  ┆ Spiders │\n",
       "│ 18  ┆ Alan   ┆ Alan   ┆ Ghosts  │\n",
       "│ 28  ┆ Alan   ┆ Alan   ┆ Ghosts  │\n",
       "│ 18  ┆ Alan   ┆ Alan   ┆ Zombies │\n",
       "│ 28  ┆ Alan   ┆ Alan   ┆ Zombies │\n",
       "│ 28  ┆ Glory  ┆ Glory  ┆ Buffy   │\n",
       "└─────┴────────┴────────┴─────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "def hash_join(table1: pl.DataFrame, index1, table2: pl.DataFrame, index2):\n",
    "    new_headers = []\n",
    "    for c in table1.columns:\n",
    "        if c in table2.columns:\n",
    "            new_headers.append(c + '_l')\n",
    "        else:\n",
    "            new_headers.append(c)\n",
    "    for c in table2.columns:\n",
    "        if c in table1.columns:\n",
    "            new_headers.append(c + '_r')\n",
    "        else:\n",
    "            new_headers.append(c)\n",
    "\n",
    "    table1, table2 = table1.rows(), table2.rows()\n",
    "    h = defaultdict(list)\n",
    "    # hash phase\n",
    "    for s in table1:\n",
    "        h[s[index1]].append(s)\n",
    "    # join phase\n",
    "    pprint(h)\n",
    "    res = [(s + r) for r in table2 for s in h[r[index2]]]\n",
    "\n",
    "    return pl.DataFrame._from_records(res, schema=new_headers)\n",
    "    \n",
    "\n",
    "df1 = pl.DataFrame({\n",
    "    'age': [27, 18, 28, 18, 28],\n",
    "    'name': [\"Jonah\", \"Alan\", \"Glory\", \"Popeye\", \"Alan\"]\n",
    "    })\n",
    "\n",
    "df2 = pl.DataFrame({\n",
    "    'name': [\"Jonah\", \"Jonah\", \"Alan\", \"Alan\", \"Glory\"],\n",
    "    'word': ['Whales', 'Spiders', 'Ghosts', 'Zombies', 'Buffy']\n",
    "})\n",
    "\n",
    "hash_join(df1, 1, df2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_join_with_partitions(table1: pyarrow.dataset, index1, table2: pyarrow.dataset, index2):\n",
    "    '''implement hash join that accepts table partitions\n",
    "    \n",
    "    the hash phase should wrap a for loop above `for s in table1` for all the partitions and store the join values in the hash\n",
    "    '''\n",
    "\n",
    "    hash_table = defaultdict(list)\n",
    "    result = []\n",
    "    # hash phase\n",
    "    for batch in table1.to_batches():\n",
    "        rows = pl.DataFrame._from_arrow(batch).rows()\n",
    "        for row in rows:\n",
    "            hash_table[row[index1]].append(row)\n",
    "\n",
    "    # join phase\n",
    "    for batch in table2.to_batches():\n",
    "        rows = pl.DataFrame._from_arrow(batch).rows()\n",
    "        for row in rows:\n",
    "            for entry in hash_table[row[index2]]:\n",
    "                result.append(entry + row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Merge Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import names\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "data_1 = [(names.get_first_name(), random.randint(0, 100)) for i in range(10)]\n",
    "data_2 = [(names.get_first_name(), random.randint(0, 100)) for i in range(10)]\n",
    "data_3 = [(names.get_first_name(), random.randint(0, 100)) for i in range(10)]\n",
    "data_4 = [(names.get_first_name(), random.randint(0, 100)) for i in range(10)]\n",
    "\n",
    "data_files = [data_1, data_2, data_3, data_4]\n",
    "\n",
    "# sort phase\n",
    "for data in data_files:\n",
    "    data.sort(key=lambda x: x[1])\n",
    "\n",
    "# merge phase\n",
    "def current_tuple(idx, data: list) -> list:\n",
    "    if idx < len(data):\n",
    "        return [data[idx]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "i = j = k = l = 0\n",
    "out_buffer = []\n",
    "while i < len(data_1) or j < len(data_2) or k < len(data_3) or l < len(data_4):\n",
    "    current_min = min(current_tuple(i, data_1) + current_tuple(j, data_2) + \\\n",
    "        current_tuple(k, data_3) + current_tuple(l, data_4), key=lambda x: x[1])\n",
    "    if i < len(data_1) and current_min == data_1[i]:\n",
    "        out_buffer.append(data_1[i])\n",
    "        i += 1\n",
    "    elif j < len(data_2) and current_min == data_2[j]:\n",
    "        out_buffer.append(data_2[j])\n",
    "        j += 1\n",
    "    elif k < len(data_3) and current_min == data_3[k]:\n",
    "        out_buffer.append(data_3[k])\n",
    "        k += 1\n",
    "    else:\n",
    "        out_buffer.append(data_4[l])\n",
    "        l += 1\n",
    "\n",
    "out_buffer == sorted(data_1 + data_2 + data_3 + data_4, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement with polars dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import names\n",
    "import random\n",
    "import os\n",
    "\n",
    "## setup\n",
    "random.seed(999)\n",
    "\n",
    "df1 = pl.DataFrame({\n",
    "    'name': [names.get_first_name() for i in range(10)],\n",
    "    'age': [random.randint(0, 100) for i in range(10)]\n",
    "})\n",
    "df2 = pl.DataFrame({\n",
    "    'name': [names.get_first_name() for i in range(10)],\n",
    "    'age': [random.randint(0, 100) for i in range(10)]\n",
    "})\n",
    "df3 = pl.DataFrame({\n",
    "    'name': [names.get_first_name() for i in range(10)],\n",
    "    'age': [random.randint(0, 100) for i in range(10)]\n",
    "})\n",
    "df4 = pl.DataFrame({\n",
    "    'name': [names.get_first_name() for i in range(10)],\n",
    "    'age': [random.randint(0, 100) for i in range(10)]\n",
    "})\n",
    "\n",
    "if not os.path.exists('./data_1.parquet'):\n",
    "    df1.write_parquet('./data_1.parquet')\n",
    "if not os.path.exists('./data_2.parquet'):\n",
    "    df2.write_parquet('./data_2.parquet')\n",
    "if not os.path.exists('./data_3.parquet'):\n",
    "    df3.write_parquet('./data_3.parquet')\n",
    "if not os.path.exists('./data_4.parquet'):\n",
    "    df4.write_parquet('./data_4.parquet')\n",
    "\n",
    "#############--------------###################\n",
    "\n",
    "data_files = sorted([f for f in os.listdir('.') if f.endswith('parquet')])\n",
    "schema = list(pl.read_parquet_schema(data_files[0]).keys()) # get schema/column names\n",
    "\n",
    "'''\n",
    "# sort\n",
    "for file in data_files:\n",
    "    data = pl.read_parquet('./' + file).rows()\n",
    "    data.sort(key=lambda x: x[1])\n",
    "    data = pl.DataFrame(data, schema=schema)\n",
    "    if not os.path.exists('./' + file.split('.')[0] + '_sorted.parquet'):\n",
    "        data.write_parquet('./' + file.split('.')[0] + '_sorted.parquet')\n",
    "'''\n",
    "\n",
    "# merge\n",
    "n_buffers = 4\n",
    "len_file = 10\n",
    "batch_size = len_file / (n_buffers + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pf1 = pq.ParquetFile('./data_1_sorted.parquet')\n",
    "iterator_1 = pf1.iter_batches(batch_size=1)\n",
    "\n",
    "pf2 = pq.ParquetFile('./data_2_sorted.parquet')\n",
    "iterator_2 = pf2.iter_batches(batch_size=1)\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "\n",
    "data_1, data_2 = next(iterator_1, False), next(iterator_2, False)\n",
    "\n",
    "while data_1 and data_2:\n",
    "    current_min = min(data_1.column('age')[0].as_py(), data_2.column('age')[0].as_py())\n",
    "    if current_min == (data_1.column('age')[0].as_py()):\n",
    "        merged_data.append(pl.from_arrow(data_1).rows()[0])\n",
    "        data_1 = next(iterator_1, False)\n",
    "    else:\n",
    "        merged_data.append(pl.from_arrow(data_1).rows()[0])\n",
    "        data_2 = next(iterator_2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jamie', 11),\n",
       " ('Jamie', 11),\n",
       " ('Jamie', 11),\n",
       " ('Christina', 21),\n",
       " ('Christina', 21),\n",
       " ('Clarence', 23),\n",
       " ('Clarence', 23),\n",
       " ('Elsie', 26),\n",
       " ('Chad', 42),\n",
       " ('Chad', 42),\n",
       " ('Chad', 42),\n",
       " ('Chad', 42),\n",
       " ('Gladys', 64),\n",
       " ('Jennifer', 70),\n",
       " ('Jennifer', 70),\n",
       " ('Rocco', 76),\n",
       " ('Rocco', 76),\n",
       " ('Jean', 82)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (18, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_0</th><th>column_1</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Jamie&quot;</td><td>11</td></tr><tr><td>&quot;Jamie&quot;</td><td>11</td></tr><tr><td>&quot;Jamie&quot;</td><td>11</td></tr><tr><td>&quot;Christina&quot;</td><td>21</td></tr><tr><td>&quot;Christina&quot;</td><td>21</td></tr><tr><td>&quot;Clarence&quot;</td><td>23</td></tr><tr><td>&quot;Clarence&quot;</td><td>23</td></tr><tr><td>&quot;Elsie&quot;</td><td>26</td></tr><tr><td>&quot;Chad&quot;</td><td>42</td></tr><tr><td>&quot;Chad&quot;</td><td>42</td></tr><tr><td>&quot;Chad&quot;</td><td>42</td></tr><tr><td>&quot;Chad&quot;</td><td>42</td></tr><tr><td>&quot;Gladys&quot;</td><td>64</td></tr><tr><td>&quot;Jennifer&quot;</td><td>70</td></tr><tr><td>&quot;Jennifer&quot;</td><td>70</td></tr><tr><td>&quot;Rocco&quot;</td><td>76</td></tr><tr><td>&quot;Rocco&quot;</td><td>76</td></tr><tr><td>&quot;Jean&quot;</td><td>82</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (18, 2)\n",
       "┌───────────┬──────────┐\n",
       "│ column_0  ┆ column_1 │\n",
       "│ ---       ┆ ---      │\n",
       "│ str       ┆ i64      │\n",
       "╞═══════════╪══════════╡\n",
       "│ Jamie     ┆ 11       │\n",
       "│ Jamie     ┆ 11       │\n",
       "│ Jamie     ┆ 11       │\n",
       "│ Christina ┆ 21       │\n",
       "│ …         ┆ …        │\n",
       "│ Jennifer  ┆ 70       │\n",
       "│ Rocco     ┆ 76       │\n",
       "│ Rocco     ┆ 76       │\n",
       "│ Jean      ┆ 82       │\n",
       "└───────────┴──────────┘"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.DataFrame(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
