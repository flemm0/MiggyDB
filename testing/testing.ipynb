{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import pathlib\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "\n",
    "import pyarrow.compute as pc\n",
    "from pyarrow import csv\n",
    "\n",
    "import os\n",
    "import polars as pl\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 0.2697105407714844 MB\n"
     ]
    }
   ],
   "source": [
    "def get_size(path, unit='MB'):\n",
    "    size = os.path.getsize(path)\n",
    "    if unit == 'MB':\n",
    "        print(f'Size: {size / (1024 * 1024)} MB')\n",
    "    elif unit == 'GB':\n",
    "        print(f'Size: {size / (1024 * 1024 * 1024)} GB')\n",
    "    else:\n",
    "        print(f'Size: {size} B')\n",
    "\n",
    "path = './data/test/playlist_2010to2022.parquet'\n",
    "\n",
    "get_size(path=path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size: 458.2281322479248 MB\n"
     ]
    }
   ],
   "source": [
    "path = 'data/test2/audio_features.csv'\n",
    "get_size(path, 'MB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = pq.ParquetFile(path)\n",
    "nrows = pf.metadata.num_rows\n",
    "n_partitions = math.ceil(os.path.getsize(path) / (1024 ** 2) / 50)\n",
    "batch_size = nrows / n_partitions\n",
    "first_n_rows = next(pf.iter_batches(batch_size = batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow._parquet.FileMetaData object at 0x7f410c1c6660>\n",
       "  created_by: parquet-cpp-arrow version 13.0.0\n",
       "  num_columns: 15\n",
       "  num_rows: 4687104\n",
       "  num_row_groups: 5\n",
       "  format_version: 2.6\n",
       "  serialized_size: 10131"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf.metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `yield` in iterator function to avoid returning entire dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup\n",
    "import polars as pl\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "\n",
    "DATA_PATH = Path('/home/flemm0/school_stuff/USC_Fall_2023/DSCI551-Final_Project/data/')\n",
    "TEST_DB_PATH = Path(DATA_PATH / 'test')\n",
    "TEMP_DB_PATH = Path(DATA_PATH / 'temp')\n",
    "\n",
    "\n",
    "## step 1: select\n",
    "def read_table(table_name):\n",
    "    dataset = ds.dataset(TEST_DB_PATH / table_name, format='parquet')\n",
    "    for partition in dataset.files:\n",
    "        partition = Path(partition)\n",
    "        data = pq.read_table(partition)\n",
    "        yield data, partition.stem\n",
    "\n",
    "\n",
    "step = 0\n",
    "query_id = 'query_' + datetime.datetime.now().strftime(\"%y%m%d_%H%M%S\") \n",
    "query_step_dir = query_id + '_' + str(step)\n",
    "\n",
    "curr_query_path = Path(TEMP_DB_PATH / query_step_dir)\n",
    "if not curr_query_path.exists():\n",
    "    Path.mkdir(curr_query_path)\n",
    "\n",
    "for partition, name in read_table('audio_features'):\n",
    "    where = (curr_query_path / name).with_suffix('.parquet')\n",
    "    pq.write_table(table=partition, where=(curr_query_path / name).with_suffix('.parquet'))\n",
    "\n",
    "\n",
    "## step 2: where\n",
    "def filter(prev_query_path, filters):\n",
    "    dataset = ds.dataset(prev_query_path, format='parquet')\n",
    "    for partition in dataset.files:\n",
    "        partition = Path(partition)\n",
    "        data = pq.read_table(partition, filters=filters) # list of tuples e.g. ('acousticness', '<', 1)\n",
    "        yield data, partition.stem\n",
    "\n",
    "prev_query_path = curr_query_path\n",
    "step += 1\n",
    "query_step_dir = query_id + '_' + str(step)\n",
    "curr_query_path = Path(TEMP_DB_PATH / query_step_dir)\n",
    "if not curr_query_path.exists():\n",
    "    Path.mkdir(curr_query_path)\n",
    "    for partition, name in filter(prev_query_path=prev_query_path, filters=[('acousticness', '<', 1)]):\n",
    "        pq.write_table(table=partition, where=(curr_query_path / name).with_suffix('.parquet'))\n",
    "\n",
    "\n",
    "## step 3: projection\n",
    "def projection(prev_query_path, selected_cols, new_col_names):\n",
    "    '''\n",
    "    Reads intermediate query results from prev_query_path and selects only specified columns. Assigns new column names.\n",
    "    Yields filtered data partitions and partition name\n",
    "    '''\n",
    "    dataset = ds.dataset(prev_query_path, format='parquet')\n",
    "    for partition in dataset.files:\n",
    "        partition = Path(partition)\n",
    "        data = pq.read_table(partition, columns=selected_cols) # list of column names\n",
    "        data.rename_columns(new_col_names)\n",
    "        yield data, partition.stem\n",
    "\n",
    "\n",
    "columns = [['acousticness'], ['test']]\n",
    "selected_cols = columns[0]\n",
    "new_col_names = columns[1] # TODO set to selected_cols if no new names provided\n",
    "\n",
    "prev_query_path = curr_query_path\n",
    "step += 1\n",
    "query_step_dir = query_id + '_' + str(step)\n",
    "curr_query_path = Path(TEMP_DB_PATH / query_step_dir)\n",
    "if not curr_query_path.exists():\n",
    "    Path.mkdir(curr_query_path)\n",
    "for partition, name in projection(prev_query_path=prev_query_path, selected_cols=selected_cols, new_col_names=new_col_names):\n",
    "    pq.write_table(table=partition, where=(curr_query_path / name).with_suffix('.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## test \n",
    "\n",
    "dataset = ds.dataset(curr_query_path)\n",
    "for f in dataset.files:\n",
    "    data = pq.read_table(f)\n",
    "    print(pl.DataFrame._from_arrow(data))\n",
    "\n",
    "## filter works!!\n",
    "## projection works!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pyarrow` tabular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "dataset = ds.dataset(source=['data/test2/audio_features_0.parquet', 'data/test2/audio_features_1.parquet'], format='parquet')\n",
    "batches = dataset.to_batches()\n",
    "\n",
    "\n",
    "# print(f'dataset object size in main memory: {sys.getsizeof(dataset) / (2 ** 20)} MB')\n",
    "# print(f'batches object size in main memory: {sys.getsizeof(batches) / (2 ** 20)} MB')\n",
    "# print(f'first batch size in main memory: {sys.getsizeof(next(batches)) / (2 ** 20)} MB')\n",
    "\n",
    "def select_all_from_table(dataset):\n",
    "    batches = dataset.to_batches()\n",
    "    truncated_dataset = dataset.head(100)\n",
    "    head = pl.DataFrame._from_arrow(truncated_dataset)\n",
    "\n",
    "    pl.Config.set_tbl_hide_dataframe_shape(True)\n",
    "    print(f'shape: ({dataset.count_rows()}, {df.shape[1]})')\n",
    "    print(head)\n",
    "    #pl.Config.set_tbl_hide_dataframe_shape(False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1874842, 15)\n",
      "shape: (100, 15)\n",
      "┌────────────┬────────────┬────────────┬────────────┬───┬───────┬────────────┬─────────┬───────────┐\n",
      "│ isrc       ┆ acousticne ┆ danceabili ┆ duration_m ┆ … ┆ tempo ┆ time_signa ┆ valence ┆ updated_o │\n",
      "│ ---        ┆ ss         ┆ ty         ┆ s          ┆   ┆ ---   ┆ ture       ┆ ---     ┆ n         │\n",
      "│ str        ┆ ---        ┆ ---        ┆ ---        ┆   ┆ i64   ┆ ---        ┆ f64     ┆ ---       │\n",
      "│            ┆ f64        ┆ f64        ┆ i64        ┆   ┆       ┆ i64        ┆         ┆ str       │\n",
      "╞════════════╪════════════╪════════════╪════════════╪═══╪═══════╪════════════╪═════════╪═══════════╡\n",
      "│ AD4X657521 ┆ 0.906      ┆ 0.65       ┆ 296733     ┆ … ┆ 110   ┆ 4          ┆ 0.336   ┆ 2023-08-2 │\n",
      "│ 84         ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 4         │\n",
      "│            ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 09:27:00  │\n",
      "│ AEA0D19911 ┆ 0.00095    ┆ 0.621      ┆ 191989     ┆ … ┆ 140   ┆ 4          ┆ 0.346   ┆ 2023-08-2 │\n",
      "│ 70         ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 4         │\n",
      "│            ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 09:30:04  │\n",
      "│ AEA0Q20040 ┆ 0.0312     ┆ 0.692      ┆ 282904     ┆ … ┆ 140   ┆ 4          ┆ 0.225   ┆ 2023-08-2 │\n",
      "│ 08         ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 4         │\n",
      "│            ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 09:30:04  │\n",
      "│ AEA0Q20040 ┆ 0.000558   ┆ 0.516      ┆ 197904     ┆ … ┆ 148   ┆ 4          ┆ 0.204   ┆ 2023-08-2 │\n",
      "│ 09         ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 4         │\n",
      "│            ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 09:30:04  │\n",
      "│ …          ┆ …          ┆ …          ┆ …          ┆ … ┆ …     ┆ …          ┆ …       ┆ …         │\n",
      "│ ATAQ715000 ┆ 0.000225   ┆ 0.671      ┆ 345078     ┆ … ┆ 128   ┆ 4          ┆ 0.475   ┆ 2023-08-2 │\n",
      "│ 15         ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 4         │\n",
      "│            ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 09:31:35  │\n",
      "│ ATAQ715000 ┆ 0.00554    ┆ 0.658      ┆ 255000     ┆ … ┆ 128   ┆ 4          ┆ 0.22    ┆ 2023-08-2 │\n",
      "│ 17         ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 4         │\n",
      "│            ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 09:31:35  │\n",
      "│ ATAQ715000 ┆ 0.00708    ┆ 0.55       ┆ 208731     ┆ … ┆ 130   ┆ 4          ┆ 0.273   ┆ 2023-08-2 │\n",
      "│ 33         ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 4         │\n",
      "│            ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 09:31:35  │\n",
      "│ ATAQ715000 ┆ 0.00015    ┆ 0.478      ┆ 228750     ┆ … ┆ 128   ┆ 4          ┆ 0.29    ┆ 2023-08-2 │\n",
      "│ 41         ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 4         │\n",
      "│            ┆            ┆            ┆            ┆   ┆       ┆            ┆         ┆ 09:31:35  │\n",
      "└────────────┴────────────┴────────────┴────────────┴───┴───────┴────────────┴─────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "select_all_from_table(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/flemm0/school_stuff/USC_Fall_2023/DSCI551-Final_Project/data/test/audio_features/audio_features_0.parquet',\n",
       " '/home/flemm0/school_stuff/USC_Fall_2023/DSCI551-Final_Project/data/test/audio_features/audio_features_1.parquet',\n",
       " '/home/flemm0/school_stuff/USC_Fall_2023/DSCI551-Final_Project/data/test/audio_features/audio_features_2.parquet',\n",
       " '/home/flemm0/school_stuff/USC_Fall_2023/DSCI551-Final_Project/data/test/audio_features/audio_features_3.parquet',\n",
       " '/home/flemm0/school_stuff/USC_Fall_2023/DSCI551-Final_Project/data/test/audio_features/audio_features_4.parquet']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow.parquet as pq\n",
    "import pathlib\n",
    "import polars as pl\n",
    "\n",
    "base = pathlib.Path('/home/flemm0/school_stuff/USC_Fall_2023/DSCI551-Final_Project/data/test/')\n",
    "\n",
    "dataset = ds.dataset(base / 'audio_features', format='parquet')\n",
    "\n",
    "dataset.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = dataset.to_batches()\n",
    "\n",
    "nxt = next(batches)\n",
    "\n",
    "pl.DataFrame._from_arrow(nxt).rows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pq.ParquetFile(dataset.files[0]).metadata.num_row_groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hash Join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>,\n",
      "            {'Alan': [(18, 'Alan'), (28, 'Alan')],\n",
      "             'Glory': [(28, 'Glory')],\n",
      "             'Jonah': [(27, 'Jonah')],\n",
      "             'Popeye': [(18, 'Popeye')]})\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7, 4)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>age</th><th>name_l</th><th>name_r</th><th>word</th></tr><tr><td>i64</td><td>str</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>27</td><td>&quot;Jonah&quot;</td><td>&quot;Jonah&quot;</td><td>&quot;Whales&quot;</td></tr><tr><td>27</td><td>&quot;Jonah&quot;</td><td>&quot;Jonah&quot;</td><td>&quot;Spiders&quot;</td></tr><tr><td>18</td><td>&quot;Alan&quot;</td><td>&quot;Alan&quot;</td><td>&quot;Ghosts&quot;</td></tr><tr><td>28</td><td>&quot;Alan&quot;</td><td>&quot;Alan&quot;</td><td>&quot;Ghosts&quot;</td></tr><tr><td>18</td><td>&quot;Alan&quot;</td><td>&quot;Alan&quot;</td><td>&quot;Zombies&quot;</td></tr><tr><td>28</td><td>&quot;Alan&quot;</td><td>&quot;Alan&quot;</td><td>&quot;Zombies&quot;</td></tr><tr><td>28</td><td>&quot;Glory&quot;</td><td>&quot;Glory&quot;</td><td>&quot;Buffy&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7, 4)\n",
       "┌─────┬────────┬────────┬─────────┐\n",
       "│ age ┆ name_l ┆ name_r ┆ word    │\n",
       "│ --- ┆ ---    ┆ ---    ┆ ---     │\n",
       "│ i64 ┆ str    ┆ str    ┆ str     │\n",
       "╞═════╪════════╪════════╪═════════╡\n",
       "│ 27  ┆ Jonah  ┆ Jonah  ┆ Whales  │\n",
       "│ 27  ┆ Jonah  ┆ Jonah  ┆ Spiders │\n",
       "│ 18  ┆ Alan   ┆ Alan   ┆ Ghosts  │\n",
       "│ 28  ┆ Alan   ┆ Alan   ┆ Ghosts  │\n",
       "│ 18  ┆ Alan   ┆ Alan   ┆ Zombies │\n",
       "│ 28  ┆ Alan   ┆ Alan   ┆ Zombies │\n",
       "│ 28  ┆ Glory  ┆ Glory  ┆ Buffy   │\n",
       "└─────┴────────┴────────┴─────────┘"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "def hash_join(table1: pl.DataFrame, index1, table2: pl.DataFrame, index2):\n",
    "    new_headers = []\n",
    "    for c in table1.columns:\n",
    "        if c in table2.columns:\n",
    "            new_headers.append(c + '_l')\n",
    "        else:\n",
    "            new_headers.append(c)\n",
    "    for c in table2.columns:\n",
    "        if c in table1.columns:\n",
    "            new_headers.append(c + '_r')\n",
    "        else:\n",
    "            new_headers.append(c)\n",
    "\n",
    "    table1, table2 = table1.rows(), table2.rows()\n",
    "    h = defaultdict(list)\n",
    "    # hash phase\n",
    "    for s in table1:\n",
    "        h[s[index1]].append(s)\n",
    "    # join phase\n",
    "    pprint(h)\n",
    "    res = [(s + r) for r in table2 for s in h[r[index2]]]\n",
    "\n",
    "    return pl.DataFrame._from_records(res, schema=new_headers)\n",
    "    \n",
    "\n",
    "df1 = pl.DataFrame({\n",
    "    'age': [27, 18, 28, 18, 28],\n",
    "    'name': [\"Jonah\", \"Alan\", \"Glory\", \"Popeye\", \"Alan\"]\n",
    "    })\n",
    "\n",
    "df2 = pl.DataFrame({\n",
    "    'name': [\"Jonah\", \"Jonah\", \"Alan\", \"Alan\", \"Glory\"],\n",
    "    'word': ['Whales', 'Spiders', 'Ghosts', 'Zombies', 'Buffy']\n",
    "})\n",
    "\n",
    "hash_join(df1, 1, df2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_join_with_partitions(table1: pyarrow.dataset, index1, table2: pyarrow.dataset, index2):\n",
    "    '''implement hash join that accepts table partitions\n",
    "    \n",
    "    the hash phase should wrap a for loop above `for s in table1` for all the partitions and store the join values in the hash\n",
    "    '''\n",
    "\n",
    "    hash_table = defaultdict(list)\n",
    "    result = []\n",
    "    # hash phase\n",
    "    for batch in table1.to_batches():\n",
    "        rows = pl.DataFrame._from_arrow(batch).rows()\n",
    "        for row in rows:\n",
    "            hash_table[row[index1]].append(row)\n",
    "\n",
    "    # join phase\n",
    "    for batch in table2.to_batches():\n",
    "        rows = pl.DataFrame._from_arrow(batch).rows()\n",
    "        for row in rows:\n",
    "            for entry in hash_table[row[index2]]:\n",
    "                result.append(entry + row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External Merge Sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import names\n",
    "import random\n",
    "\n",
    "random.seed(123)\n",
    "data_1 = [(names.get_first_name(), random.randint(0, 100)) for i in range(10)]\n",
    "data_2 = [(names.get_first_name(), random.randint(0, 100)) for i in range(10)]\n",
    "data_3 = [(names.get_first_name(), random.randint(0, 100)) for i in range(10)]\n",
    "data_4 = [(names.get_first_name(), random.randint(0, 100)) for i in range(10)]\n",
    "\n",
    "data_files = [data_1, data_2, data_3, data_4]\n",
    "\n",
    "# sort phase\n",
    "for data in data_files:\n",
    "    data.sort(key=lambda x: x[1])\n",
    "\n",
    "# merge phase\n",
    "def current_tuple(idx, data: list) -> list:\n",
    "    if idx < len(data):\n",
    "        return [data[idx]]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "i = j = k = l = 0\n",
    "out_buffer = []\n",
    "while i < len(data_1) or j < len(data_2) or k < len(data_3) or l < len(data_4):\n",
    "    current_min = min(current_tuple(i, data_1) + current_tuple(j, data_2) + \\\n",
    "        current_tuple(k, data_3) + current_tuple(l, data_4), key=lambda x: x[1])\n",
    "    if i < len(data_1) and current_min == data_1[i]:\n",
    "        out_buffer.append(data_1[i])\n",
    "        i += 1\n",
    "    elif j < len(data_2) and current_min == data_2[j]:\n",
    "        out_buffer.append(data_2[j])\n",
    "        j += 1\n",
    "    elif k < len(data_3) and current_min == data_3[k]:\n",
    "        out_buffer.append(data_3[k])\n",
    "        k += 1\n",
    "    else:\n",
    "        out_buffer.append(data_4[l])\n",
    "        l += 1\n",
    "\n",
    "out_buffer == sorted(data_1 + data_2 + data_3 + data_4, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implement with polars dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import names\n",
    "import random\n",
    "import os\n",
    "\n",
    "## setup\n",
    "random.seed(999)\n",
    "\n",
    "df1 = pl.DataFrame({\n",
    "    'name': [names.get_first_name() for i in range(10)],\n",
    "    'age': [random.randint(0, 100) for i in range(10)]\n",
    "})\n",
    "df2 = pl.DataFrame({\n",
    "    'name': [names.get_first_name() for i in range(10)],\n",
    "    'age': [random.randint(0, 100) for i in range(10)]\n",
    "})\n",
    "df3 = pl.DataFrame({\n",
    "    'name': [names.get_first_name() for i in range(10)],\n",
    "    'age': [random.randint(0, 100) for i in range(10)]\n",
    "})\n",
    "df4 = pl.DataFrame({\n",
    "    'name': [names.get_first_name() for i in range(10)],\n",
    "    'age': [random.randint(0, 100) for i in range(10)]\n",
    "})\n",
    "\n",
    "if not os.path.exists('./data_1.parquet'):\n",
    "    df1.write_parquet('./data_1.parquet')\n",
    "if not os.path.exists('./data_2.parquet'):\n",
    "    df2.write_parquet('./data_2.parquet')\n",
    "if not os.path.exists('./data_3.parquet'):\n",
    "    df3.write_parquet('./data_3.parquet')\n",
    "if not os.path.exists('./data_4.parquet'):\n",
    "    df4.write_parquet('./data_4.parquet')\n",
    "\n",
    "#############--------------###################\n",
    "\n",
    "data_files = sorted([f for f in os.listdir('.') if f.endswith('parquet')])\n",
    "schema = list(pl.read_parquet_schema(data_files[0]).keys()) # get schema/column names\n",
    "\n",
    "'''\n",
    "# sort\n",
    "for file in data_files:\n",
    "    data = pl.read_parquet('./' + file).rows()\n",
    "    data.sort(key=lambda x: x[1])\n",
    "    data = pl.DataFrame(data, schema=schema)\n",
    "    if not os.path.exists('./' + file.split('.')[0] + '_sorted.parquet'):\n",
    "        data.write_parquet('./' + file.split('.')[0] + '_sorted.parquet')\n",
    "'''\n",
    "\n",
    "# merge\n",
    "n_buffers = 4\n",
    "len_file = 10\n",
    "batch_size = len_file / (n_buffers + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "pf1 = pq.ParquetFile('./data_1_sorted.parquet')\n",
    "iterator_1 = pf1.iter_batches(batch_size=1)\n",
    "\n",
    "pf2 = pq.ParquetFile('./data_2_sorted.parquet')\n",
    "iterator_2 = pf2.iter_batches(batch_size=1)\n",
    "\n",
    "merged_data = []\n",
    "\n",
    "\n",
    "data_1, data_2 = next(iterator_1, False), next(iterator_2, False)\n",
    "\n",
    "while data_1 and data_2:\n",
    "    current_min = min(data_1.column('age')[0].as_py(), data_2.column('age')[0].as_py())\n",
    "    if current_min == (data_1.column('age')[0].as_py()):\n",
    "        merged_data.append(pl.from_arrow(data_1).rows()[0])\n",
    "        data_1 = next(iterator_1, False)\n",
    "    else:\n",
    "        merged_data.append(pl.from_arrow(data_1).rows()[0])\n",
    "        data_2 = next(iterator_2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jamie', 11),\n",
       " ('Jamie', 11),\n",
       " ('Jamie', 11),\n",
       " ('Christina', 21),\n",
       " ('Christina', 21),\n",
       " ('Clarence', 23),\n",
       " ('Clarence', 23),\n",
       " ('Elsie', 26),\n",
       " ('Chad', 42),\n",
       " ('Chad', 42),\n",
       " ('Chad', 42),\n",
       " ('Chad', 42),\n",
       " ('Gladys', 64),\n",
       " ('Jennifer', 70),\n",
       " ('Jennifer', 70),\n",
       " ('Rocco', 76),\n",
       " ('Rocco', 76),\n",
       " ('Jean', 82)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (18, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>column_0</th><th>column_1</th></tr><tr><td>str</td><td>i64</td></tr></thead><tbody><tr><td>&quot;Jamie&quot;</td><td>11</td></tr><tr><td>&quot;Jamie&quot;</td><td>11</td></tr><tr><td>&quot;Jamie&quot;</td><td>11</td></tr><tr><td>&quot;Christina&quot;</td><td>21</td></tr><tr><td>&quot;Christina&quot;</td><td>21</td></tr><tr><td>&quot;Clarence&quot;</td><td>23</td></tr><tr><td>&quot;Clarence&quot;</td><td>23</td></tr><tr><td>&quot;Elsie&quot;</td><td>26</td></tr><tr><td>&quot;Chad&quot;</td><td>42</td></tr><tr><td>&quot;Chad&quot;</td><td>42</td></tr><tr><td>&quot;Chad&quot;</td><td>42</td></tr><tr><td>&quot;Chad&quot;</td><td>42</td></tr><tr><td>&quot;Gladys&quot;</td><td>64</td></tr><tr><td>&quot;Jennifer&quot;</td><td>70</td></tr><tr><td>&quot;Jennifer&quot;</td><td>70</td></tr><tr><td>&quot;Rocco&quot;</td><td>76</td></tr><tr><td>&quot;Rocco&quot;</td><td>76</td></tr><tr><td>&quot;Jean&quot;</td><td>82</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (18, 2)\n",
       "┌───────────┬──────────┐\n",
       "│ column_0  ┆ column_1 │\n",
       "│ ---       ┆ ---      │\n",
       "│ str       ┆ i64      │\n",
       "╞═══════════╪══════════╡\n",
       "│ Jamie     ┆ 11       │\n",
       "│ Jamie     ┆ 11       │\n",
       "│ Jamie     ┆ 11       │\n",
       "│ Christina ┆ 21       │\n",
       "│ …         ┆ …        │\n",
       "│ Jennifer  ┆ 70       │\n",
       "│ Rocco     ┆ 76       │\n",
       "│ Rocco     ┆ 76       │\n",
       "│ Jean      ┆ 82       │\n",
       "└───────────┴──────────┘"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.DataFrame(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
